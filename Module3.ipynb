{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "It is necessary to transform raw text into something a ML algo can digest and this is the mainn objective of text  pre processing. \n",
    "The main steps of text pre processing are as follows:\n",
    "1. Remove punctuations\n",
    "2. Tokenization\n",
    "3. Remove stopwords\n",
    "4. Lemmatization\n",
    "5. Stemming\n",
    "6. Remove numbers\n",
    "7. Lowercasing\n",
    "8. Remove special characters\n",
    "9. Remove extra spaces\n",
    "10. Remove URLs\n",
    "11. Remove HTML tags\n",
    "12. Remove emojis\n",
    "13. Remove emoticons\n",
    "14. Remove non-ASCII characters\n",
    "15. Remove multiple whitespaces\n",
    "16. etc\n",
    "\n",
    "\n",
    "1. Tokenization\n",
    "    Tokenization is the process of breaking the text into smaller parts called tokens. Tokens can be words, sentences, or subwords. It is an    essential step in both NLP and text analysis. Tokenization is the first step in text analytics. The process of tokenization is divided into two types: word tokenization and sentence tokenization. Word tokenization is the process of splitting the text into individual words. Sentence tokenization is the process of splitting the text into individual sentences.\n",
    "\n",
    "2. Text Normalization\n",
    "    Text normalization is the process of transforming text into a single canonical form. This is done by converting text to lowercase, removing special characters, removing extra spaces, removing numbers, removing stopwords, and so on. Text normalization is an essential step in text analysis. It helps in reducing the size of the vocabulary and improving the performance of the model.\n",
    "\n",
    "3. Part of Speech tagging\n",
    "    Part of speech tagging is the process of assigning a part of speech to each word in a sentence. The part of speech indicates the role of the word in the sentence. There are eight parts of speech: noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection. Part of speech tagging is an essential step in text analysis. It helps in understanding the grammatical structure of the text and extracting useful information from it.\n",
    "\n",
    "4. Dependency Parsing\n",
    "    Dependency parsing is the process of analyzing the grammatical structure of a sentence. It involves identifying the relationships between words in a sentence and representing them as a tree structure. Dependency parsing is an essential step in text analysis. It helps in understanding the syntactic structure of the text and extracting useful information from it.\n",
    "\n",
    "About the module:\n",
    "- Corpus, Tokens, Ngram\n",
    "- Tokenization\n",
    "- Stemming \n",
    "- Lemmatization\n",
    "- Stopwords\n",
    "- Part of Speech Tagging\n",
    "- Dependency parsing\n",
    "\n",
    "1. Corpus : Collection of text documents, Corpus > Documents > Paragraphs > Sentences > Tokens\n",
    "\n",
    "2. Tokens : Words or sentences or basically smallest unit of a text.\n",
    "\n",
    "3. Ngram : A contiguous sequence of n items from a given sample of text or speech i.e combination of N words/characters together.\n",
    "    a. Unigrams(n=1) : I, Love, my, phone, very, much\n",
    "    b. Bigrams(n=2) : I Love, Love my, my phone, phone very, very much\n",
    "    c. Trigrams(n=3) : I Love my, Love my phone, my phone very, phone very much\n",
    "\n",
    "4. Tokenization: Process of splitting a text object into smaller units. There are many Tokenization techniques and many tokenizers are present\n",
    "    there. For example White Space Tokenizer, Unigram Tokenizer, Regex Tokenizer etc\n",
    "\n",
    "5. Normalization: Morpheme is defined as base form of a word. Structure of token: <prefix> <morpheme> <suffix>\n",
    "    Example: Antinationalist -> Anti National ist\n",
    "    Process of converting Sa token into its base form, helpful in reducing data dimensionality and text cleaning. Types: Stemminng and Lemmatization\n",
    "\n",
    "#### Stemming: \n",
    "    Process of reducing inflected words to their word stem, base or root form. It removes suffices or prefixes from a word and reduce it to its root word. \n",
    "    Example: Entitling -> Entitle, Entitled -> Entitle, Entitles -> Entitle\n",
    "    Types: Porter Stemmer, Snowball Stemmer, Lancaster Stemmer\n",
    "    Advantages: Fast, Easy to implement, Reduces data dimensionality\n",
    "    Disadvantages: Overstemming, Produces non-existant words, Produces words that are not semantically correct\n",
    "\n",
    "    Sometimes the stemmed word is not a regular word and doesnt have any meaning.\n",
    "    Example: Entitling -> Entitl, Entitled -> Entitl, Entitles -> Entitl\n",
    "\n",
    "#### Lemmatization:\n",
    "    Process of converting a word into its base form. It is similar to stemming but it is more powerful. It reduces the word into its base form based on the dictionary meaning of a word. It makes use of vocabulary and morphological analysis of words. \n",
    "    Example: Entitling -> Entitle, Entitled -> Entitle, Entitles -> Entitle\n",
    "    Advantages: Produces real words, Reduces data dimensionality, Reduces overstemming\n",
    "    Disadvantages: Slow, Complex, Requires a dictionary\n",
    "\n",
    "    Lemmatization is preferred over stemming because lemmatization does morphological analysis of the words. It is more sophisticated than stemming. It reduces the word into its base form based on the dictionary meaning of a word. It makes use of vocabulary and morphological analysis of words.\n",
    "\n",
    "6. Stopwords: \n",
    "Words that are very common in the text and provide no useful information. They are removed from the text before processing. Example: is, am, are, the, a, an, in, on, at, to, etc\n",
    "\n",
    "\n",
    "7. Part of Speech Tagging: \n",
    "Defines the syntactic context and role of words in a sentence. Process of assigning a part of speech to each word in a sentence. The part of speech indicates the role of the word in the sentence. There are eight parts of speech: noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection. Part of speech tagging is an essential step in text analysis. It helps in understanding the grammatical structure of the text and extracting useful information from it. It is defined by their relations with adjacent words and ML or Rule Based Processes are Used.\n",
    "\n",
    "* Used of POS Tags:\n",
    "    - text cleaning \n",
    "    - feature engineering\n",
    "    - word sense disambiguation\n",
    "    - sentiment analysis\n",
    "\n",
    "8. Constituency Grammar: \n",
    "Constituents : Words/ Phrases/ group of words\n",
    "    - Noun Phrase(NP) : A phrase that has a noun as its head. Example: The big dog\n",
    "    - Verb Phrase(VP) : A phrase that has a verb as its head. Example: The dog is barking\n",
    "    - Adjective Phrase(AP) : A phrase that has an adjective as its head. Example: The big dog\n",
    "    - Adverb Phrase(ADVP) : A phrase that has an adverb as its head. Example: The dog is barking loudly\n",
    "    - Prepositional Phrase(PP) : A phrase that has a preposition as its head. Example: The dog is in the house\n",
    "    - Sentence(S) : A sentence is a group of words that expresses a complete thought. Example: The dog is barking loudly in the house\n",
    "    - Clause(C) : A clause is a group of words that contains a subject and a verb. Example: The dog is barking loudly in\n",
    "\n",
    "9. Dependency Grammar:\n",
    "Dependency Parsing is the process of analyzing the grammatical structure of a sentence. It involves identifying the relationships between words in a sentence and representing them as a tree structure. Dependency parsing is an essential step in text analysis. It helps in understanding the syntactic structure of the text and extracting useful information from it. It is used in Named Entity Recognition, Sentiment Analysis, Question Answering, Machine Translation, etc.\n",
    "In short all the words are directly or indirectly linked to roots of the words in a sentence.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = \"Hi John, How are you doing? I will be going to the market. Let's Catchup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'John',\n",
       " ',',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'I',\n",
       " 'will',\n",
       " 'be',\n",
       " 'going',\n",
       " 'to',\n",
       " 'the',\n",
       " 'market',\n",
       " '.',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'Catchup']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi John, How are you doing?',\n",
       " 'I will be going to the market.',\n",
       " \"Let's Catchup\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n",
      "play\n",
      "increas\n",
      "live\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "\n",
    "print(ps.stem(\"playing\"))\n",
    "print(ps.stem(\"increases\")) \n",
    "print(ps.stem(\"lively\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "pythoner\n",
      "pythoning\n",
      "pythoned\n",
      "pythonly\n",
      "increase\n",
      "run\n",
      "running\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(lemmatizer.lemmatize(w))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"increases\"))\n",
    "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"running\", pos=\"n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sahib Preet\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sahib Preet\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'NNP'),\n",
       " ('John', 'NNP'),\n",
       " (',', ','),\n",
       " ('How', 'NNP'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('doing', 'VBG'),\n",
       " ('?', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('market', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Let', 'VB'),\n",
       " (\"'s\", 'POS'),\n",
       " ('Catchup', 'NNP')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POSTagging\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hi John, How are you doing? I will be going to the market. Let's Catchup\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('book.n.01'),\n",
       " Synset('book.n.02'),\n",
       " Synset('record.n.05'),\n",
       " Synset('script.n.01'),\n",
       " Synset('ledger.n.01'),\n",
       " Synset('book.n.06'),\n",
       " Synset('book.n.07'),\n",
       " Synset('koran.n.01'),\n",
       " Synset('bible.n.01'),\n",
       " Synset('book.n.10'),\n",
       " Synset('book.n.11'),\n",
       " Synset('book.v.01'),\n",
       " Synset('reserve.v.04'),\n",
       " Synset('book.v.03'),\n",
       " Synset('book.v.04')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Wordnet\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet.synsets(\"Book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'love')\n",
      "('love', 'to')\n",
      "('to', 'play')\n",
      "('play', 'cricket')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = \"I love to play cricket\"\n",
    "\n",
    "n = 2\n",
    "\n",
    "for i in ngrams(word_tokenize(sentence), n):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
